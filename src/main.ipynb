{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIR Project - AI Mario\n",
    "This jupyter notebook contains the application of nueral network and reinforcement learning algorithms learnt from the tutorials to simulate Mario completing a variety of levels in a Super Mario Bros pybullet gym environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mario Environment\n",
    "We use a Super Mario Bros environment (https://pypi.org/project/gym-super-mario-bros/) with a continuous state space and discrete action space. The goal of this activity is to complete Mario levels as fast as possible while also achieving a high level score. Episodes end when Mario reaches the end of the level, if Mario dies, or if a certain time as elapsed.\n",
    "\n",
    "### Action Space\n",
    "- 0: No Movement\n",
    "- 1: Move Right\n",
    "- 2: Move Right + Jump\n",
    "- 3: Move Right + Speed Up\n",
    "- 4: Move Right + Jump + Speed Up\n",
    "- 5: Jump\n",
    "- 6: Move Left\n",
    "- 7: Move Left + Jump\n",
    "- 8: Move Left + Speed Up\n",
    "- 9: Move Left + Jump + Speed Up\n",
    "- 10: Down\n",
    "- 11: Up\n",
    "\n",
    "### Observation Space\n",
    "The info dictionary returned by step contains the following:\n",
    "| Key | Unit | Description |\n",
    "| --- | ---- | ----------- |\n",
    "| coins | int | Number of collected coins |\n",
    "| flag_get | bool | True if Mario reached a flag |\n",
    "| life | int | Number of lives left |\n",
    "| score | int | Cumulative in-game score |\n",
    "| stage | int | Current stage |\n",
    "| status | str | Mario's status/power |\n",
    "| time | int | Time left on the clock |\n",
    "| world | int | Current world |\n",
    "| x_pos | int | Mario's x position in the stage |\n",
    "| y_pos | int | Mario's y position in the stage |\n",
    "\n",
    "### Rewards\n",
    "| Feature | Description | Value when Positive | Value when Negative | Value when Equal |\n",
    "|---------|-------------|---------------------|---------------------|------------------|\n",
    "| Difference in agent x values between states | Controls agent's movement | Moving right | Moving left | Not moving |\n",
    "| Time difference in the game clock between frames | Prevents agent from staying still | - | Clock ticks | Clock doesn't tick |\n",
    "| Death Penalty | Discourages agent from death | - | Agent dead | Agent alive |\n",
    "| Coins | Encourages agent to get coins | Coin collected | - | No coin collected |\n",
    "| Score | Encourages agent to get higher score | Score Value | Score Value | Score Value |\n",
    "| Flag | Encourages agent to reach middle & end flag | Flag collected | - | Flag not collected |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    }
   ],
   "source": [
    "# Importing required python packages\n",
    "import pybullet as p\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Importing required Super Mario Bros packages\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Importing required directory packages for the models\n",
    "from ReinforcementLearning import ReinforcementLearning\n",
    "from Wrappers import apply_wrappers\n",
    "\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0' \n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "# Function to display the testing video of the agent in the juypyter notebook\n",
    "def display_video(frames, framerate=30):\n",
    "  \"\"\"Generates video from `frames`.\n",
    "\n",
    "  Args:\n",
    "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
    "    framerate (int): Frame rate in units of Hz.\n",
    "\n",
    "  Returns:\n",
    "    Display object.\n",
    "  \"\"\"\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparmaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY = False                 # Sets the environment render mode to human or rgb_array\n",
    "CUSTOM_REWARDS = True           # Sets state to include custom rewards\n",
    "EPISODES = 15000                # Number of episodes to train the AI on\n",
    "MEM_SIZE = 50000                # Size of the memory in replay buffer\n",
    "REPLAY_START_SIZE = 5000        # Amount of samples to fill the replay buffer before training\n",
    "NETWORK_UPDATE_ITER = 2000      # Number of iterations before learning func updates the Q weights\n",
    "\n",
    "MEM_RETAIN = 0.1                # Size of memory that cannot be overwritten (avoids catastrophic forgetting)\n",
    "BATCH_SIZE = 32                 # Size of random batches when sampling experiences\n",
    "LEARNING_RATE = 0.00025         # Learning rate for optimizing neural network weights\n",
    "GAMMA = 0.9                     # Discount factor for future rewards\n",
    "EPSILON_START = 0.2             # Starting exploration rate\n",
    "EPSILON_END = 0.1               # Ending exploration rate\n",
    "EPSILON_DECAY = (EPSILON_START - EPSILON_END) / (0.05 * EPISODES)   # Rate at which exploration rate decays\n",
    "\n",
    "DQN_DIM1 = 512                  # Number of neurons in DQN's first hidden layer\n",
    "DQN_DIM2 = 512                  # Number of neurons in DQN's second hidden layer\n",
    "\n",
    "# Creating variables to store evaluation metrics\n",
    "episode_history = []\n",
    "episode_reward_history = []\n",
    "episode_loss_history = []\n",
    "episode_length_history = []\n",
    "episode_success_history = []\n",
    "episode_coin_history = []\n",
    "episode_score_history = []\n",
    "\n",
    "average_episode_reward_history = []\n",
    "average_episode_loss_history = []\n",
    "average_episode_length_history = []\n",
    "average_episode_success_history = []\n",
    "average_episode_coin_history = []\n",
    "average_episode_score_history = []\n",
    "\n",
    "episode_batch_loss = 0\n",
    "episode_batch_score = 0\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "episode_loss = 0\n",
    "successful_episodes = 0\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to shape the environment rewards with our custom rewards\n",
    "def shapeRewards(info, prev_info, done):\n",
    "    # Creating constants for reward shaping\n",
    "    COIN_REWARD = 1\n",
    "    SCORE_REWARD = 0.1\n",
    "    FLAG_REWARD = 1000\n",
    "    RIGHT_REWARD = 5\n",
    "    LEFT_REWARD = -7\n",
    "    DEATH_REWARD = -1000\n",
    "    TIME_REWARD = -10\n",
    "\n",
    "    # Checking if death reward should be added\n",
    "    death_reward = 0\n",
    "    if done and info['flag_get'] == False:\n",
    "        death_reward = DEATH_REWARD\n",
    "\n",
    "    # Applying the movement reward\n",
    "    x_pos_dif = info['x_pos'] - prev_info['x_pos']\n",
    "    right_reward = 0  # Setting default reward\n",
    "    if x_pos_dif > 0:\n",
    "        right_reward = RIGHT_REWARD * x_pos_dif\n",
    "    elif x_pos_dif < 0:\n",
    "        right_reward = LEFT_REWARD * abs(x_pos_dif)\n",
    "\n",
    "    # Checking if any custom rewards should be added\n",
    "    coin_reward = COIN_REWARD * (info['coins'] - prev_info['coins'])\n",
    "    score_reward = SCORE_REWARD * (info['score'] - prev_info['score'])\n",
    "    flag_reward = FLAG_REWARD if info['flag_get'] else 0\n",
    "    time_reward = TIME_REWARD * (prev_info['time'] - info['time'])\n",
    "\n",
    "    # Calculating the custom reward\n",
    "    custom_reward = coin_reward + score_reward + flag_reward + right_reward + death_reward + time_reward\n",
    "    return custom_reward  # Returning the custom reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path for saving the model policy network\n",
    "save_path = os.path.join(os.getcwd(), \"policy_network.pkl\")\n",
    "\n",
    "# Ensuring a CUDA GPU device is available\n",
    "if torch.cuda.is_available():\n",
    "    # Creating device variable and setting it to the CUDA device\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device: \", torch.cuda.get_device_name(0))  # Printing CUDA device name\n",
    "else:\n",
    "    print(\"CUDA GPU is not available\")\n",
    "\n",
    "# Load the saved model\n",
    "saved_model = torch.load(save_path)\n",
    "# Load history data\n",
    "with open('history_data1.pkl', 'rb') as f:\n",
    "    (episode_history, average_episode_reward_history, average_episode_loss_history, average_episode_length_history, average_episode_success_history, average_episode_coin_history, average_episode_score_history) = pickle.load(f)\n",
    "\n",
    "# Creating the Super Mario Bros environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', render_mode='human' if DISPLAY else 'rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "env = apply_wrappers(env)  # Applying the custom wrapper for the environment\n",
    "\n",
    "# Creating DQN Reinforcement Learning Agent\n",
    "agent = ReinforcementLearning(env, MEM_SIZE, MEM_RETAIN, BATCH_SIZE, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, NETWORK_UPDATE_ITER, REPLAY_START_SIZE, DQN_DIM1, DQN_DIM2)\n",
    "agent.policy_network.load_state_dict(saved_model)\n",
    "\n",
    "# Initialising and resetting the environment\n",
    "env.reset()\n",
    "state, reward, done, trunc, info = env.step(action=0)\n",
    "\n",
    "# Looping through the episodes to train the model\n",
    "for episode in range(10000, 15000):\n",
    "    done = False  # Setting default done state\n",
    "    prev_info = None  # Setting previous info to None\n",
    "    state, info = env.reset()  # Resetting environment and getting state\n",
    "\n",
    "    # Running the episode until done (indicated by death or by reaching the goal)\n",
    "    while not done:\n",
    "        # Sampling an action to take within the environment\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Adding custom rewards to the environment\n",
    "        if CUSTOM_REWARDS and prev_info is not None:\n",
    "            custom_reward = shapeRewards(info, prev_info, done)\n",
    "            reward += custom_reward  # Adding custom reward to environment default reward\n",
    "\n",
    "        # Adding the experience to the memory and checking if we should train the model\n",
    "        agent.memory.add(state, action, reward, state_, done)  # Adding experience to replay buffer\n",
    "        if agent.memory.memory_count >= REPLAY_START_SIZE:\n",
    "            loss = agent.learn()  # Updating Q-value weights of the neural network\n",
    "            episode_batch_loss += loss  # Updating batch loss\n",
    "            episode_loss += loss  # Updating episode loss\n",
    "\n",
    "        state = state_  # Updating current state\n",
    "        prev_info = info  # Updating previous info\n",
    "\n",
    "        # Updating performance metrics\n",
    "        episode_reward += reward  # Updating episode reward\n",
    "        episode_batch_score += reward  # Update episode score\n",
    "        episode_length += 1 # Updating episode length\n",
    "\n",
    "    # Appending episodes and rewards to the history for performance metrics\n",
    "    print(\"Epsiode: \", episode, \" Reward: \", episode_reward)\n",
    "    if info['flag_get'] == True:\n",
    "            successful_episodes += 1  # Updating successful episodes counter\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    episode_loss_history.append(episode_loss)\n",
    "    episode_length_history.append(episode_length)\n",
    "    episode_coin_history.append(info['coins'])\n",
    "    episode_score_history.append(info['score'])\n",
    "\n",
    "    episode_reward = 0  # Resetting episode reward\n",
    "    episode_length = 0  # Resetting episode length\n",
    "    episode_loss = 0  # Resetting episode loss\n",
    "\n",
    "    # Saving model every batches of 100 episodes\n",
    "    if episode % 100 == 0 and agent.memory.memory_count > REPLAY_START_SIZE:\n",
    "        torch.save(agent.policy_network.state_dict(), save_path)\n",
    "        print(\"Average total reward per episode batch since episode \", episode, \": \", episode_batch_score/ float(100))\n",
    "        print(\"Average total loss per episode batch since episode \", episode, \": \", episode_batch_loss/ float(100))\n",
    "        episode_batch_score = 0  # Resetting batch score for next batch\n",
    "        episode_batch_loss = 0  # Resetting batch loss for next batch\n",
    "\n",
    "    # Updating the histories every 500 episodes\n",
    "    if episode % 500 == 0:\n",
    "        episode_history.append(episode)\n",
    "        average_episode_reward_history.append(sum(episode_reward_history[-500:]) / 500)\n",
    "        average_episode_loss_history.append(sum(episode_loss_history[-500:]) / 500)\n",
    "        average_episode_length_history.append(sum(episode_length_history[-500:]) / 500)\n",
    "        average_episode_coin_history.append(sum(episode_coin_history[-500:]) / 500)\n",
    "        average_episode_score_history.append(sum(episode_score_history[-500:]) / 500)\n",
    "        average_episode_success_history.append(successful_episodes)\n",
    "        successful_episodes = 0\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history data\n",
    "with open('history_data1.pkl', 'wb') as f:\n",
    "    pickle.dump((episode_history, average_episode_reward_history, average_episode_loss_history, average_episode_length_history, average_episode_success_history, average_episode_coin_history, average_episode_score_history), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load history data\n",
    "with open('history_data1.pkl', 'rb') as f:\n",
    "    (episode_history, average_episode_reward_history, average_episode_loss_history, average_episode_length_history, average_episode_success_history, average_episode_coin_history, average_episode_score_history) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting episode reward history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, average_episode_reward_history)\n",
    "plt.title('Reward History')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Total Reward')\n",
    "plt.show()\n",
    "\n",
    "# Plotting episode loss history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, average_episode_loss_history)\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Total Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plotting episode length history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, average_episode_length_history)\n",
    "plt.title('Episode Length History')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Total Length')\n",
    "plt.show()\n",
    "\n",
    "# Plotting episode success history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, average_episode_success_history)\n",
    "plt.title('Successful Episode History')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Successful Episodes')\n",
    "plt.show()\n",
    "\n",
    "# Plotting episode coins history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, average_episode_coin_history)\n",
    "plt.title('Number of Coins per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Number of Coins')\n",
    "plt.show()\n",
    "\n",
    "# Plotting episode score history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, average_episode_score_history)\n",
    "plt.title('Score per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Super Mario Bros environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', render_mode='human' if DISPLAY else 'rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "env = apply_wrappers(env)  # Applying the custom wrapper for the environment\n",
    "\n",
    "# Creating DQN Reinforcement Learning Agent\n",
    "agent = ReinforcementLearning(env, MEM_SIZE, MEM_RETAIN, BATCH_SIZE, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, NETWORK_UPDATE_ITER, REPLAY_START_SIZE, DQN_DIM1, DQN_DIM2)\n",
    "\n",
    "# Initialising and resetting the environment\n",
    "state, info = env.reset()\n",
    "state, reward, done, trunc, info = env.step(action=0)\n",
    "\n",
    "# Loading the saved model policy network\n",
    "agent.policy_network.load_state_dict(torch.load(\"policy_network.pkl\"))  # Loading policy network\n",
    "\n",
    "frames = []  # Frames container for video\n",
    "frames.append(env.render())  # Appending initial frame to video\n",
    "agent.policy_network.eval()  # Setting policy network to evaluation mode\n",
    "\n",
    "# Running the episode until done (indicated by death or by reaching the goal)\n",
    "while True:\n",
    "    # Getting the Q-values from the trained neural network\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(agent.memory.device)\n",
    "        q_values = agent.policy_network(state_tensor)\n",
    "\n",
    "    # Taking steps in the environment based on the Q-values for the given state\n",
    "    action = torch.argmax(q_values).item()\n",
    "    state, reward, done, trunc, info = env.step(action)\n",
    "    frames.append(np.copy(env.render()))  # Appending frame to video\n",
    "\n",
    "    # Breaking the loop if the episode is done\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()  # Closing the environment\n",
    "display_video(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
