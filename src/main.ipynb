{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIR Project - AI Mario\n",
    "This jupyter notebook contains the application of nueral network and reinforcement learning algorithms learnt from the tutorials to simulate Mario completing a variety of levels in a Super Mario Bros pybullet gym environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mario Environment\n",
    "We use a Super Mario Bros environment (https://pypi.org/project/gym-super-mario-bros/) with a continuous state space and discrete action space. The goal of this activity is to complete Mario levels as fast as possible while also achieving a high level score. Episodes end when Mario reaches the end of the level, if Mario dies, or if a certain time as elapsed.\n",
    "\n",
    "### Action Space\n",
    "- 0: No Movement\n",
    "- 1: Move Right\n",
    "- 2: Move Right + Jump\n",
    "- 3: Move Right + Speed Up\n",
    "- 4: Move Right + Jump + Speed Up\n",
    "- 5: Jump\n",
    "- 6: Move Left\n",
    "- 7: Move Left + Jump\n",
    "- 8: Move Left + Speed Up\n",
    "- 9: Move Left + Jump + Speed Up\n",
    "- 10: Down\n",
    "- 11: Up\n",
    "\n",
    "### Observation Space\n",
    "The info dictionary returned by step contains the following:\n",
    "| Key | Unit | Description |\n",
    "| --- | ---- | ----------- |\n",
    "| coins | int | Number of collected coins |\n",
    "| flag_get | bool | True if Mario reached a flag |\n",
    "| life | int | Number of lives left |\n",
    "| score | int | Cumulative in-game score |\n",
    "| stage | int | Current stage |\n",
    "| status | str | Mario's status/power |\n",
    "| time | int | Time left on the clock |\n",
    "| world | int | Current world |\n",
    "| x_pos | int | Mario's x position in the stage |\n",
    "| y_pos | int | Mario's y position in the stage |\n",
    "\n",
    "### Rewards\n",
    "| Feature | Description | Value when Positive | Value when Negative | Value when Equal |\n",
    "|---------|-------------|---------------------|---------------------|------------------|\n",
    "| Difference in agent x values between states | Controls agent's movement | Moving right | Moving left | Not moving |\n",
    "| Time difference in the game clock between frames | Prevents agent from staying still | - | Clock ticks | Clock doesn't tick |\n",
    "| Death Penalty | Discourages agent from death | - | Agent dead | Agent alive |\n",
    "| Coins | Encourages agent to get coins | Coin collected | - | No coin collected |\n",
    "| Score | Encourages agent to get higher score | Score Value | Score Value | Score Value |\n",
    "| Flag | Encourages agent to reach middle & end flag | Flag collected | - | Flag not collected |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required python packages\n",
    "import pybullet as p\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Importing required Super Mario Bros packages\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Importing required directory packages for the models\n",
    "from ReinforcementLearning import ReinforcementLearning\n",
    "from Wrappers import apply_wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparmaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY = True                  # Sets the environment render mode to human or rgb_array\n",
    "CUSTOM_REWARDS = True           # Sets state to include custom rewards\n",
    "EPISODES = 10000                # Number of episodes to train the AI on\n",
    "MEM_SIZE = 100000               # Size of the memory in replay buffer\n",
    "REPLAY_START_SIZE = 1000        # Amount of samples to fill the replay buffer before training\n",
    "NETWORK_UPDATE_ITER = 1000      # Number of iterations before learning func updates the Q weights\n",
    "\n",
    "MEM_RETAIN = 0.1                # Size of memory that cannot be overwritten (avoids catastrophic forgetting)\n",
    "BATCH_SIZE = 32                 # Size of random batches when sampling experiences\n",
    "LEARNING_RATE = 0.00025         # Learning rate for optimizing neural network weights\n",
    "GAMMA = 0.9                     # Discount factor for future rewards\n",
    "EPSILON_START = 1.0             # Starting exploration rate\n",
    "EPSILON_END = 0.1               # Ending exploration rate\n",
    "EPSILON_DECAY = 0.99999975      # Rate at which exploration rate decays\n",
    "\n",
    "DQN_DIM1 = 512                  # Number of neurons in DQN's first hidden layer\n",
    "DQN_DIM2 = 512                  # Number of neurons in DQN's second hidden layer\n",
    "\n",
    "# Creating variables to store evaluation metrics\n",
    "episode_history = []\n",
    "episode_reward_history = []\n",
    "episode_loss_history = []\n",
    "episode_length_history = []\n",
    "episode_success_history = []\n",
    "episode_coin_history = []\n",
    "episode_score_history = []\n",
    "\n",
    "episode_batch_loss = 0\n",
    "episode_batch_score = 0\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "episode_loss = 0\n",
    "successful_episodes = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to shape the environment rewards with our custom rewards\n",
    "def shapeRewards(info, prev_info):\n",
    "    # Creating constants for reward shaping\n",
    "    COIN_REWARD = 0.1\n",
    "    SCORE_REWARD = 1\n",
    "    FLAG_REWARD = 50\n",
    "\n",
    "    # Checking if any custom rewards should be added\n",
    "    coin_reward = COIN_REWARD if info['coins'] > prev_info['coins'] else 0\n",
    "    score_reward = SCORE_REWARD * (info['score'] - prev_info['score'])\n",
    "    flag_reward = FLAG_REWARD if info['flag_get'] else 0\n",
    "    custom_reward = coin_reward + score_reward + flag_reward\n",
    "\n",
    "    # Normalising the reward into range (-15, 15)\n",
    "    custom_reward = max(min(custom_reward, 15), -15)\n",
    "    return custom_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path for saving the model policy network\n",
    "save_path = os.path.join(os.getcwd(), \"policy_network.pkl\")\n",
    "\n",
    "# Ensuring a CUDA GPU device is available\n",
    "if torch.cuda.is_available():\n",
    "    # Creating device variable and setting it to the CUDA device\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device: \", torch.cuda.get_device_name(0))  # Printing CUDA device name\n",
    "else:\n",
    "    print(\"CUDA GPU is not available\")\n",
    "\n",
    "# Creating the Super Mario Bros environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', render_mode='human' if DISPLAY else 'rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "env = apply_wrappers(env)  # Applying the custom wrapper for the environment\n",
    "\n",
    "# Creating DQN Reinforcement Learning Agent\n",
    "agent = ReinforcementLearning(env, MEM_SIZE, MEM_RETAIN, BATCH_SIZE, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, NETWORK_UPDATE_ITER, REPLAY_START_SIZE, DQN_DIM1, DQN_DIM2)\n",
    "\n",
    "# Initialising and resetting the environment\n",
    "env.reset()\n",
    "state, reward, done, trunc, info = env.step(action=0)\n",
    "\n",
    "# Looping through the episodes to train the model\n",
    "for episode in range(EPISODES):\n",
    "    done = False  # Setting default done state\n",
    "    prev_info = None\n",
    "    state, info = env.reset()  # Resetting environment and getting state\n",
    "\n",
    "    # Running the episode until done (indicated by death or by reaching the goal)\n",
    "    while not done:\n",
    "        # Sampling an action to take within the environment\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Adding custom rewards to the environment\n",
    "        if CUSTOM_REWARDS and prev_info is not None:\n",
    "            custom_reward = shapeRewards(info, prev_info)\n",
    "            reward += custom_reward  # Adding custom reward to environment default reward\n",
    "\n",
    "        # Adding the experience to the memory and checking if we should train the model\n",
    "        agent.memory.add(state, action, reward, state_, done)  # Adding experience to replay buffer\n",
    "        if agent.memory.memory_count >= REPLAY_START_SIZE:\n",
    "            episode_loss = agent.learn()  # Updating Q-value weights of the neural network\n",
    "            episode_batch_loss += episode_loss  # Updating batch loss\n",
    "\n",
    "        state = state_  # Updating current state\n",
    "        prev_info = info  # Updating previous info\n",
    "\n",
    "        # Updating performance metrics\n",
    "        if info['flag_get'] == True:\n",
    "            successful_episodes += 1  # Updating successful episodes counter\n",
    "        episode_reward += reward  # Updating episode reward\n",
    "        episode_batch_score += reward  # Update episode score\n",
    "        episode_length += 1 # Updating episode length\n",
    "\n",
    "    # Appending episodes and rewards to the history for performance metrics\n",
    "    print(\"Epsiode: \", episode, \" Reward: \", episode_reward)\n",
    "    episode_history.append(episode)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    episode_loss_history.append(episode_loss)\n",
    "    episode_length_history.append(episode_length)\n",
    "    episode_success_history.append(successful_episodes)\n",
    "    episode_coin_history.append(info['coins'])\n",
    "    episode_score_history.append(info['score'])\n",
    "    episode_reward = 0  # Resetting episode reward\n",
    "    episode_length = 0  # Resetting episode length\n",
    "\n",
    "    # Saving model every batches of 100 episodes\n",
    "    if episode % 100 == 0 and agent.memory.memory_count > REPLAY_START_SIZE:\n",
    "        torch.save(agent.policy_network.state_dict(), save_path)\n",
    "        print(\"Average total reward per episode batch since episode \", episode, \": \", episode_batch_score/ float(100))\n",
    "        print(\"Average total loss per episode batch since episode \", episode, \": \", episode_batch_loss/ float(100))\n",
    "        episode_batch_score = 0  # Resetting batch score for next batch\n",
    "        episode_batch_loss = 0  # Resetting batch loss for next batch\n",
    "\n",
    "print(\"Number of successful episodes: \", successful_episodes)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting episode reward history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, episode_reward_history)\n",
    "plt.title('Reward History')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()\n",
    "\n",
    "# Plotting episode loss history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, episode_loss_history)\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plotting episode length history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, episode_length_history)\n",
    "plt.title('Episode Length History')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Length')\n",
    "plt.show()\n",
    "\n",
    "# Plotting episode success history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, episode_success_history)\n",
    "plt.title('Successful Episode History')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Successful Episodes')\n",
    "plt.show()\n",
    "\n",
    "# Plotting episode coins history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, episode_coin_history)\n",
    "plt.title('Number of Coins per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of Coins')\n",
    "plt.show()\n",
    "\n",
    "# Plotting episode score history\n",
    "plt.figure()\n",
    "plt.plot(episode_history, episode_score_history)\n",
    "plt.title('Score per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Super Mario Bros environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', render_mode='human' if DISPLAY else 'rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "env = apply_wrappers(env)  # Applying the custom wrapper for the environment\n",
    "\n",
    "# Creating DQN Reinforcement Learning Agent\n",
    "agent = ReinforcementLearning(env, MEM_SIZE, MEM_RETAIN, BATCH_SIZE, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, NETWORK_UPDATE_ITER, REPLAY_START_SIZE, DQN_DIM1, DQN_DIM2)\n",
    "\n",
    "# Initialising and resetting the environment\n",
    "state, info = env.reset()\n",
    "state, reward, done, trunc, info = env.step(action=0)\n",
    "\n",
    "# Loading the saved model policy network\n",
    "agent.policy_network.load_state_dict(torch.load(\"policy_network.pkl\"))  # Loading policy network\n",
    "\n",
    "frames = []  # Frames container for video\n",
    "frames.append(env.render())  # Appending initial frame to video\n",
    "agent.policy_network.eval()  # Setting policy network to evaluation mode\n",
    "\n",
    "# Running the episode until done (indicated by death or by reaching the goal)\n",
    "while True:\n",
    "    # Getting the Q-values from the trained neural network\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(agent.memory.device)\n",
    "        q_values = agent.policy_network(state_tensor)\n",
    "\n",
    "    # Taking steps in the environment based on the Q-values for the given state\n",
    "    action = torch.argmax(q_values).item()\n",
    "    state, reward, done, trunc, info = env.step(action)\n",
    "    frames.append(np.copy(env.render()))  # Appending frame to video\n",
    "\n",
    "    # Breaking the loop if the episode is done\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()  # Closing the environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
