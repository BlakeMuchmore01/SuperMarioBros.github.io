{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    }
   ],
   "source": [
    "# Importing required python packages\n",
    "import pybullet as p\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Importing required Super Mario Bros packages\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Importing required directory packages for the models\n",
    "from ReinforcementLearning import ReinforcementLearning\n",
    "from Wrappers import apply_wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparmaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY = True                  # Sets the environment render mode to human or rgb_array\n",
    "EPISODES = 50000                # Number of episodes to train the AI on\n",
    "MEM_SIZE = 100000               # Size of the memory in replay buffer\n",
    "REPLAY_START_SIZE = 5000        # Amount of samples to fill the replay buffer before training\n",
    "NETWORK_UPDATE_ITER = 5000      # Number of iterations before learning func updates the Q weights\n",
    "\n",
    "MEM_RETAIN = 0.1                # Size of memory that cannot be overwritten (avoids catastrophic forgetting)\n",
    "BATCH_SIZE = 32                 # Size of random batches when sampling experiences\n",
    "LEARNING_RATE = 0.00025         # Learning rate for optimizing neural network weights\n",
    "GAMMA = 0.9                     # Discount factor for future rewards\n",
    "EPSILON_START = 1.0             # Starting exploration rate\n",
    "EPSILON_END = 0.1               # Ending exploration rate\n",
    "EPSILON_DECAY = 0.99999975      # Rate at which exploration rate decays\n",
    "\n",
    "DQN_DIM1 = 512                  # Number of neurons in DQN's first hidden layer\n",
    "DQN_DIM2 = 512                  # Number of neurons in DQN's second hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables to store evaluation metrics\n",
    "episode_history = []\n",
    "episode_reward_history = []\n",
    "episode_batch_score = 0\n",
    "\n",
    "# Setting path for saving the model policy network\n",
    "save_path = os.path.join(os.getcwd(), \"policy_network.pkl\")\n",
    "\n",
    "# Ensuring a CUDA GPU device is available\n",
    "if torch.cuda.is_available():\n",
    "    # Creating device variable and setting it to the CUDA device\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device: \", torch.cuda.get_device_name(0))  # Printing CUDA device name\n",
    "else:\n",
    "    print(\"CUDA GPU is not available\")\n",
    "\n",
    "# Creating the Super Mario Bros environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', render_mode='human' if DISPLAY else 'rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "env = apply_wrappers(env)  # Applying the custom wrapper for the environment\n",
    "\n",
    "# Creating DQN Reinforcement Learning Agent\n",
    "agent = ReinforcementLearning(env, MEM_SIZE, MEM_RETAIN, BATCH_SIZE, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, NETWORK_UPDATE_ITER, REPLAY_START_SIZE, DQN_DIM1, DQN_DIM2)\n",
    "\n",
    "# Initialising and resetting the environment\n",
    "env.reset()\n",
    "state, reward, done, trunc, info = env.step(action=0)\n",
    "\n",
    "# Looping through the episodes to train the model\n",
    "for episode in range(EPISODES):\n",
    "    done = False  # Setting default done state\n",
    "    state, info = env.reset()  # Resetting environment and getting state\n",
    "\n",
    "    # Running the episode until done (indicated by death or by reaching the goal)\n",
    "    while not done:\n",
    "        # Sampling an action to take within the environment\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Adding the experience to the memory and checking if we should train the model\n",
    "        agent.memory.add(state, action, reward, state_, done)  # Adding experience to replay buffer\n",
    "        if agent.memory.memory_count >= REPLAY_START_SIZE:\n",
    "            agent.learn()  # Updating Q-value weights of the neural network\n",
    "\n",
    "        state = state_  # Updating current state\n",
    "        prev_info = info  # Updating previous info\n",
    "\n",
    "        # Saving model every batches of 100 episodes\n",
    "        if episode % 100 == 0 and agent.memory.memory_count > REPLAY_START_SIZE:\n",
    "            torch.save(agent.policy_network.state_dict(), save_path)\n",
    "            print(\"Average total reward per episode batch since episode \", episode, \": \", episode_batch_score/ float(100))\n",
    "            episode_batch_score = 0  # Resetting batch score for next batch\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Super Mario Bros environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', render_mode='human' if DISPLAY else 'rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "env = apply_wrappers(env)  # Applying the custom wrapper for the environment\n",
    "\n",
    "# Creating DQN Reinforcement Learning Agent\n",
    "agent = ReinforcementLearning(env, MEM_SIZE, MEM_RETAIN, BATCH_SIZE, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, NETWORK_UPDATE_ITER, REPLAY_START_SIZE, DQN_DIM1, DQN_DIM2)\n",
    "\n",
    "# Initialising and resetting the environment\n",
    "state, info = env.reset()\n",
    "state, reward, done, trunc, info = env.step(action=0)\n",
    "\n",
    "# Loading the saved model policy network\n",
    "agent.policy_network.load_state_dict(torch.load(\"policy_network.pkl\"))  # Loading policy network\n",
    "\n",
    "frames = []  # Frames container for video\n",
    "frames.append(env.render())  # Appending initial frame to video\n",
    "agent.policy_network.eval()  # Setting policy network to evaluation mode\n",
    "\n",
    "# Running the episode until done (indicated by death or by reaching the goal)\n",
    "while True:\n",
    "    # Getting the Q-values from the trained neural network\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(agent.memory.device)\n",
    "        q_values = agent.policy_network(state_tensor)\n",
    "\n",
    "    # Taking steps in the environment based on the Q-values for the given state\n",
    "    action = torch.argmax(q_values).item()\n",
    "    state, reward, done, trunc, info = env.step(action)\n",
    "    frames.append(np.copy(env.render()))  # Appending frame to video\n",
    "\n",
    "    # Breaking the loop if the episode is done\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()  # Closing the environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
